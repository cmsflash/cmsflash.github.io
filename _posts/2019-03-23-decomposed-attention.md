---
layout: post
title:  "Decomposed Attention: Self-Attention with Linear Complexities"
date:   2019-03-23 22:26:11 +0800
categories: AI
---

*Decomposed Attention: Self-Attention with Linear Complexities* is a work by me and my colleagues at SenseTime. In the work, we proposed a simple but effective method to decrease the computational and memory complexities of the self-attention mechanism from quadratic to linear, without loss of accuracy. The paper is available on [arXiv](https://arxiv.org/abs/1812.01243). Enjoy it.
